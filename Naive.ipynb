{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14b0225b",
   "metadata": {},
   "source": [
    "### 나이브 베이즈 분류기 (Naive Bayes Classifiers) \n",
    "- 나이브 베이즈 분류기는 선형 모델과 매우 유사  \n",
    "- LogisticRegression 이나 LinearSVC 같은 선형 분류기보다 훈련 속도는 빠르고 일반화 성능은 조금 떨어짐  \n",
    "- 나이브 베이즈 분류기가 효과적인 이유는 각 feature를 개별로 취급해 파라미터를 학습하고 각 feature에서 클래스별 통계를 단순하게 취합하기 때문  \n",
    "- scikit-learn 에 구현된 나이브 베이즈 분류기는 GaussianNB, BernoulliNB, MultinomiaNB 세 가지  \n",
    "- GaussianNB 는 연속적인 어떤 데이터에도 적용할 수 있음  \n",
    "- BernoulliNB 는 이진 데이터에 적용  \n",
    "- MultinomiaNB 는 카운트 데이터(feature가 어떤 것을 헤아린 정수 카운트로, 예를 들면 문장에 나타난 단어의 수)에 적용  \n",
    "- BernoulliNB, MultinomiaNB 는 대부분 텍스트 데이터를 분류할 때 사용  \n",
    "- BernoulliNB 분류기는 각 클래스의 feature 중 0이 아닌 것이 몇 개인지 센다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50eecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X=np.array([[0,1,0,1],[1,0,1,1],[0,0,0,1],[1,0,1,0]])\n",
    "y=np.array([0,1,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee960142",
   "metadata": {},
   "source": [
    "- 이진 feature 4개를 가진 데이터 포인트 4개 있음, 클래스는 0과 1 두 개  \n",
    "- 출력 y의 클래스가 0인 경우, 첫 번째 feature는 0이 두 번이고 0이 아닌 것은 없으며  \n",
    "- 두번째 feature는 0이 한 번 1도 한 번  \n",
    "- 같은 방식으로 두 번째 클래스에 해당하는 데이터 포인트에 대해서도 계산  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e569deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature counts:\n",
      " {0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\n"
     ]
    }
   ],
   "source": [
    "#0이 아닌 원소를 세는 과정 \n",
    "counts = {}\n",
    "\n",
    "for label in np.unique(y):\n",
    "    # iterate over each class\n",
    "    # count (sum) entries of 1 per feature\n",
    "    counts[label] = X[y == label].sum(axis=0)\n",
    "    \n",
    "print(\"Feature counts:\\n\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86476fb",
   "metadata": {},
   "source": [
    "- 다른 두 나이브 베이즈 모델 GaussianNB, MultinomiaNB 은 계산하는 통계 데이터의 종류가 조금 다름  \n",
    "- MultinomiaNB 는 클래스별로 feature의 평균을 계산하고 GaussianNB 은 클래스별로 각 feature의 표준편차와 평균을 저장함  \n",
    "- 예측할 땐 데이터 포인트를 클래스의 통계 값과 비교해서 가장 잘 맞는 클래스를 예측값으로 함  \n",
    "- BernoulliNB, MultinomiaNB 의 예측 공식은 선형 모델과 형태가 같음  \n",
    "- 그러나 나이브 베이즈 모델의 coef_ 는 기울기 w가 아니라서 선형 모델과는 의미가 다름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f183c08",
   "metadata": {},
   "source": [
    "### 장단점과 매개변수  \n",
    "- BernoulliNB, MultinomiaNB 는 모델의 복잡도를 조절하는 alpha 매개변수 하나를 가짐  \n",
    "- alpha가 주어지면 알고리즘이 모든 feature에 양의 값을 가진 가상의 데이터 포인트를 alpha 수 만큼 추가함, 이는 통계  데이터를 완벽하게 만들어 줌  \n",
    "- alpha가 크면 더 완만해지고 모델의 복잡도는 낮아짐, alpha에 따른 알고리즘 성능 변동은 비교적 크지 않아서 alpha 값이 성능 향상에 크게 기여하지 않음  \n",
    "- GaussianNB 은 대부분 매우 고차원적인 데이터셋에 사용하고, 다른 두 나이브 베이즈 모델은 텍스트 같은 희소한 데이터를 카운트하는 데 사용함  \n",
    "- MultinomiaNB 는 보통 0이 아닌 feature가 비교적 많은 데이터셋에서 BernoulliNB 보다 성능이 높음  \n",
    "- 나이브 베이즈 모델과 선형 모델의 장단점은 비슷함  \n",
    "- 선형 모델로는 학습 시간이 너무 오래 걸리는 매우 큰 데이터셋에는 나이브 베이즈 모델을 시도해 볼 만하며 종종 사용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01b0648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
